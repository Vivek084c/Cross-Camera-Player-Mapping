{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81c20c35",
   "metadata": {},
   "source": [
    "## understanding the pose estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f43fdada",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved pose frame: output_pose/pose_frame_0.jpg\n",
      "Saved pose frame: output_pose/pose_frame_1.jpg\n",
      "Saved pose frame: output_pose/pose_frame_2.jpg\n",
      "Saved pose frame: output_pose/pose_frame_3.jpg\n",
      "Saved pose frame: output_pose/pose_frame_4.jpg\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Load YOLOv8 pose estimation model\n",
    "model = YOLO(\"yolov8x-pose.pt\")\n",
    "\n",
    "# Create output directory\n",
    "output_dir = \"output_pose\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Open the tacticam video\n",
    "video_path = \"dataset/tacticam.mp4\"\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "frame_idx = 0\n",
    "max_frames = 5\n",
    "\n",
    "while frame_idx < max_frames:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"End of video or error reading frame.\")\n",
    "        break\n",
    "\n",
    "    # Run pose detection\n",
    "    results = model.predict(source=frame, save=False, conf=0.1, verbose=False)[0]\n",
    "\n",
    "    # Plot keypoints on frame\n",
    "    annotated_frame = results.plot()\n",
    "\n",
    "    # Save annotated frame\n",
    "    save_path = os.path.join(output_dir, f\"pose_frame_{frame_idx}.jpg\")\n",
    "    cv2.imwrite(save_path, annotated_frame)\n",
    "    print(f\"Saved pose frame: {save_path}\")\n",
    "\n",
    "    frame_idx += 1\n",
    "\n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e0bb628",
   "metadata": {},
   "source": [
    "## above code does not give pose estimation as the model is not able to pick small scale humans"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec818240",
   "metadata": {},
   "source": [
    "## use fine tuneed yolo 11 to get player and then run pose estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae4d56b3",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "from ultralytics import YOLO\n",
    "from torchreid.utils import FeatureExtractor\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Load fine-tuned player detector\n",
    "detector = YOLO(\"model/best.pt\")\n",
    "\n",
    "# Load pose estimation model\n",
    "pose_model = YOLO(\"yolov8x-pose.pt\")\n",
    "\n",
    "# Load ReID model\n",
    "extractor = FeatureExtractor(\n",
    "    model_name='osnet_x1_0',\n",
    "    model_path='osnet_x1_0_market1501.pth',\n",
    "    device='cpu'\n",
    ")\n",
    "\n",
    "# Video input/output\n",
    "video_path = \"dataset/broadcast.mp4\"\n",
    "output_dir = \"output_pose\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "# Parameters\n",
    "frame_count = 0\n",
    "max_frames = 5\n",
    "font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "global_player_id = 0\n",
    "embed_array = []\n",
    "\n",
    "while frame_count < max_frames:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"End of video or frames.\")\n",
    "        break\n",
    "\n",
    "    # Step 1: Detect players using fine-tuned YOLO\n",
    "    detection_result = detector.predict(frame, conf=0.4, save=False, verbose=False)[0]\n",
    "    boxes = detection_result.boxes\n",
    "    annotated_frame = frame.copy()\n",
    "\n",
    "    if boxes is not None:\n",
    "        for i, box in enumerate(boxes):\n",
    "            x1, y1, x2, y2 = map(int, box.xyxy[0].tolist())\n",
    "            cls_id = int(box.cls[0].item())\n",
    "            class_name = detector.names[cls_id]\n",
    "\n",
    "            if class_name.lower() == \"player\":\n",
    "                # Draw detection\n",
    "                cv2.rectangle(annotated_frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "                crop = frame[y1:y2, x1:x2]\n",
    "                crop_rgb = cv2.cvtColor(crop, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "                # Step 2: Extract embedding\n",
    "                emb = extractor(crop_rgb)\n",
    "                emb = F.normalize(emb, p=2, dim=1)\n",
    "\n",
    "                cx, cy = (x1 + x2) / 2, (y1 + y2) / 2\n",
    "                matched_id = None\n",
    "                best_score = 0\n",
    "\n",
    "                for saved_emb, pid, (prev_cx, prev_cy) in embed_array:\n",
    "                    sim = torch.mm(emb, saved_emb.t()).item()\n",
    "                    spatial_dist = np.sqrt((cx - prev_cx)**2 + (cy - prev_cy)**2)\n",
    "                    box_diag = np.sqrt((x2 - x1)**2 + (y2 - y1)**2)\n",
    "                    norm_dist = spatial_dist / (box_diag + 1e-6)\n",
    "                    final_score = (sim * 6 + (1 - norm_dist)) / 7\n",
    "\n",
    "                    if final_score > best_score and final_score > 0.7:\n",
    "                        best_score = final_score\n",
    "                        matched_id = pid\n",
    "\n",
    "                if matched_id is not None:\n",
    "                    for idx, (e, pid, pos) in enumerate(embed_array):\n",
    "                        if pid == matched_id:\n",
    "                            new_emb = F.normalize(e * 0.6 + emb * 0.4, p=2, dim=1)\n",
    "                            embed_array[idx] = (new_emb, matched_id, (cx, cy))\n",
    "                            break\n",
    "                    player_id = matched_id\n",
    "                else:\n",
    "                    player_id = global_player_id\n",
    "                    embed_array.append((emb, player_id, (cx, cy)))\n",
    "                    global_player_id += 1\n",
    "\n",
    "                label_text = f\"ID: {player_id}, Conf: {box.conf[0]:.2f}\"\n",
    "                cv2.putText(annotated_frame, label_text, (x1, y1 - 10), font, 0.6, (0, 255, 0), 2)\n",
    "\n",
    "                # Step 3: Pose estimation on full frame\n",
    "                pose_results = pose_model.predict(frame, conf=0.1, save=False, verbose=False)[0]\n",
    "\n",
    "                if pose_results.keypoints is not None:\n",
    "                    # Draw pose on the annotated frame\n",
    "                    annotated_frame = pose_results.plot(img=annotated_frame)\n",
    "\n",
    "    # Save the annotated frame\n",
    "    out_path = os.path.join(output_dir, f\"pose_frame_{frame_count}.jpg\")\n",
    "    cv2.imwrite(out_path, annotated_frame)\n",
    "    print(f\"Saved: {out_path}\")\n",
    "\n",
    "    frame_count += 1\n",
    "\n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "816a9095",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
